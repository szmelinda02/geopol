{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Fellegi_Aron\\venvs\\geopol-env\\lib\\site-packages\\fuzzywuzzy\\fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "from itertools import combinations\n",
    "from collections import Counter\n",
    "from fuzzywuzzy import fuzz\n",
    "from utils.get_dates import * \n",
    "\n",
    "#from langdetect import detect\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from utils.nlp_utils import NLPClass\n",
    "\n",
    "nlp_class = NLPClass()\n",
    "TOPIC = 'immigrants'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{TOPIC}/temp/article_urls.pkl', 'rb') as f:\n",
    "    urls = pickle.load(f)\n",
    "    \n",
    "df = pd.DataFrame(columns=['search_terms', 'url'])\n",
    "\n",
    "\n",
    "for key, values in urls.items():\n",
    "    for value in values:\n",
    "        row = [key, value]\n",
    "        df.loc[len(df)] = row\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [00:05<00:00,  1.94it/s]\n"
     ]
    }
   ],
   "source": [
    "df['article_infos'] = df.url.progress_apply(lambda x: nlp_class.get_article_infos(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 3324.59it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 5029.74it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 32.69it/s]\n"
     ]
    }
   ],
   "source": [
    "df['text'] = df.article_infos.progress_apply(lambda x: x.text if x is not None else None)\n",
    "df['publish_date'] = df.article_infos.progress_apply(lambda x: x.publish_date if x is not None else None)\n",
    "del df['article_infos']\n",
    "df['split_text'] = df['text'].progress_apply(lambda x: nlp_class.split_text(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 50/50 [00:16<00:00,  3.03it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 50/50 [00:18<00:00,  2.70it/s]\n"
     ]
    }
   ],
   "source": [
    "df_paragraph = df.explode('split_text').reset_index(drop=True)\n",
    "df_paragraph.drop('text',inplace=True, axis=1)\n",
    "\n",
    "#delete  line below if you want to run the full process\n",
    "df_paragraph = df_paragraph.head(50)\n",
    "df_paragraph['entities'] = df_paragraph.split_text.progress_apply(lambda x: nlp_class.return_ner_labels(x[0:10000]))\n",
    "df_paragraph['sentiment'] = df_paragraph['split_text'].progress_apply(lambda x: nlp_class.sentiment_scorer(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df_paragraph['keyword'] = df_paragraph.search_terms.apply(lambda x: x[0])\n",
    "df_paragraph['site'] = df_paragraph.search_terms.apply(lambda x: x[1])\n",
    "\n",
    "columns_order = ['keyword', 'site', 'url', 'publish_date', 'sentiment', 'split_text', 'entities']\n",
    "df_paragraph = df_paragraph[columns_order]\n",
    "\n",
    "missing_urls_df = df_paragraph[df_paragraph.publish_date.isnull()]\n",
    "not_missing = df_paragraph[df_paragraph.publish_date.isnull() == False]\n",
    "not_missing['publish_date'] = not_missing.publish_date.apply(lambda x: x.date())\n",
    "\n",
    "missing_urls_dates_dict = {}\n",
    "\n",
    "for url in tqdm(missing_urls_df['url'].unique()):\n",
    "    date = get_date_for_url(url)\n",
    "    missing_urls_dates_dict.update({url: date})\n",
    "\n",
    "missing_urls_df['publish_date'] = missing_urls_df.url.apply(lambda x: missing_urls_dates_dict[x])\n",
    "#missing_urls_df['publish_date'] = missing_urls_df.publish_date.apply(lambda x: x.date() if x is not None and type(x) != str else None)\n",
    "df = pd.concat([not_missing, missing_urls_df])\n",
    "df['publish_date'] = df.publish_date.apply(lambda x: x.date() if type(x) == datetime else x)\n",
    "df['publish_date'] = df.publish_date.apply(lambda x: None if x == '' else x)\n",
    "df['persons_ner'] = df.entities.apply(lambda entity_list: [x[0] for x in entity_list if x[1] == 'PERSON' ])\n",
    "#df['persons_ner'] = df['persons_ner'].apply(lambda x: x[:-3] if len(x)>3 else [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 78/78 [00:00<00:00, 7915.12it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 50/50 [00:00<00:00, 4578.63it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "LIMIT = 0\n",
    "person_cnt = [x for x in Counter([item for sublist in list(df['persons_ner']) for item in sublist]).most_common() if x[1] > LIMIT]\n",
    "persons = list(set([item.lower() for sublist in list(df['persons_ner']) for item in sublist if item in set([x[0] for x in person_cnt])]))\n",
    "\n",
    "duplications = []\n",
    "for name_tuple in tqdm(set(combinations(persons, 2))):\n",
    "    #if len(name_tuple[0]) < 3 or len(name_tuple[1]) < 3:\n",
    "    #    continue\n",
    "    similarity = fuzz.token_set_ratio(name_tuple[0], name_tuple[1])\n",
    "    if similarity == 100:\n",
    "        if name_tuple[0] == name_tuple[1]:\n",
    "            continue\n",
    "        duplications.append(sorted(name_tuple, key = len))\n",
    "\n",
    "duplication_dict = {x[0] : [] for x in duplications }\n",
    "for name in duplication_dict.keys():\n",
    "    duplication_dict[name] = [x[1] for x in duplications if x[0] == name]\n",
    "\n",
    "normalization_dict = {}\n",
    "\n",
    "for key, value in duplication_dict.items():\n",
    "    if len(value) == 1:\n",
    "        normalization_dict[key] = value[0]\n",
    "    elif len(value) > 1:\n",
    "        reference_value = sorted(value, key = len, reverse = True)[0]\n",
    "\n",
    "        #for new_key in sorted(value, key = len)[1:]:\n",
    "        #    flattened_dict.update({new_key : reference_value})\n",
    "        normalization_dict[key] = reference_value\n",
    "\n",
    "        \n",
    "\n",
    "df['persons_ner_normalized'] = df.persons_ner.progress_apply(lambda entity_list: [normalization_dict.get(entity.lower(), entity.lower()) for entity in entity_list])\n",
    "df['persons_ner_normalized'] = df['persons_ner_normalized'].apply(lambda x: [re.sub(r\"[^a-zA-Z\\s]\", \"\", str(item).replace(\"'s\", \"\").replace(\"//n\", \"\").strip()).title() for item in x])\n",
    "df['persons_ner_normalized_unique'] = df['persons_ner_normalized'].apply(lambda x: set(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['org_ner'] = df.entities.apply(lambda entity_list: [x[0] for x in entity_list if x[1] == 'ORG' ])\n",
    "df['org_ner_unique'] = df['org_ner'].apply(lambda x: set(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_parquet(f'{TOPIC}/preprocessed_output.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geopol-env",
   "language": "python",
   "name": "geopol-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
